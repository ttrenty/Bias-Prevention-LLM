{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a large language model (LLM) using the Hugging Face `transformers` library along with LoRA (Low-Rank Adaptation) and a custom PyTorch training loop involves several steps. Here's a structured guide to achieving this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Pretrained Model and Tokenizer\n",
    "You can start by loading a pretrained model and tokenizer from Hugging Face's model hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name = 'gpt2'  # Example for GPT-2, replace with your model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"crumb/nano-mistral\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"crumb/nano-mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Integrate LoRA (Low-Rank Adaptation)\n",
    "To integrate LoRA, you'll use the `peft` library, which helps you add low-rank adapters to the model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "\n",
    "# Set up LoRA configuration\n",
    "loftq_config = LoftQConfig(loftq_bits=4)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=16,  # Scaling factor for LoRA updates\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    target_modules=[\"attn\", \"mlp\"],  # Specify layers to apply LoRA\n",
    "    init_lora_weights=\"loftq\",\n",
    "    loftq_config=loftq_config  # LoftQ configuration\n",
    ")\n",
    "\n",
    "# Convert model to PEFT (LoRA)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data\n",
    "Load your dataset (either custom or from the Hugging Face dataset hub):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Example dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenize the Dataset\n",
    "Tokenize your dataset according to your model’s tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Custom PyTorch Training Loop\n",
    "Now, let's set up a custom PyTorch training loop. We will need an optimizer, learning rate scheduler, and gradient accumulation if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define optimizer (only train LoRA parameters)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set up a learning rate scheduler (optional)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to device\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "        labels = inputs[\"input_ids\"]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save the Fine-tuned Model\n",
    "After training, save the model and tokenizer for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Considerations:\n",
    "- **LoRA Layers**: The `LoraConfig` allows you to choose which layers will benefit from LoRA adaptations (`\"attn\"`, `\"mlp\"`, etc.).\n",
    "- **Gradient Updates**: If you're only interested in fine-tuning the LoRA layers, make sure you update only the LoRA parameters in the optimizer (you can filter out the non-LoRA parameters).\n",
    "- **Distributed Training**: If you're fine-tuning on large models, consider using distributed training with `accelerate` or `deepspeed`.\n",
    "- **Mixed Precision**: For better performance, especially with large models, use mixed precision training (`torch.cuda.amp`).\n",
    "\n",
    "### Conclusion:\n",
    "This setup combines Hugging Face’s `transformers` library, LoRA integration through `peft`, and a custom PyTorch training loop. This approach lets you fine-tune large models efficiently while utilizing LoRA to save memory and computation resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
