{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a pre-trained BERT model for classification using nativeÂ PyTorch\n",
    "Competition: [Covid-19 tweet classification](https://zindi.africa/competitions/covid-19-tweet-classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "Substitute the file paths with the paths for your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/Train.csv')\n",
    "test_data = pd.read_csv('./data/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>The bitcoin halving is cancelled due to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>MercyOfAllah In good times wrapped in its gran...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>266 Days No Digital India No Murder of e learn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>India is likely to run out of the remaining RN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>In these tough times the best way to grow is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                               text  target\n",
       "0  train_0            The bitcoin halving is cancelled due to       1\n",
       "1  train_1  MercyOfAllah In good times wrapped in its gran...       0\n",
       "2  train_2  266 Days No Digital India No Murder of e learn...       1\n",
       "3  train_3  India is likely to run out of the remaining RN...       1\n",
       "4  train_4  In these tough times the best way to grow is t...       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    2746\n",
       "1    2541\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "Train size = 70% of the total size\n",
    "Test size = 30% of the total size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(data['text'], data['target'], train_size = 0.7, shuffle = True, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the tokenizer and pass the text data to get tokens that can be passed to the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer(list(train_X), padding = True, truncation=True)\n",
    "test_tokens = tokenizer(list(test_X), padding = True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3725, 9216, 8653, 2740, 124, 5187, 1559, 5465, 1545, 1367, 5966, 1580, 9493, 2740, 122, 5539, 1571, 26516, 4859, 1545, 4735, 1116, 21606, 5117, 1477, 3993, 1604, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] Update Total cases 3 017 766 12 879 Current cases 1 915 580 856 Deaths 207 722 468 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens['input_ids'][0])\n",
    "print(tokenizer.decode(train_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens['attention_mask'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenData(Dataset):\n",
    "    def __init__(self, train = False):\n",
    "        if train:\n",
    "            self.text_data = train_X\n",
    "            self.tokens = train_tokens\n",
    "            self.labels = list(train_Y)\n",
    "        else:\n",
    "            self.text_data = test_X\n",
    "            self.tokens = test_tokens\n",
    "            self.labels = list(test_Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting batch size. Create Dataset class objects for training and testing data. Declare Dataloader objects for these Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "train_dataset = TokenData(train = True)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TokenData(train = False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the train data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('input_ids', tensor([[  101,  7444,   153,  ...,     0,     0,     0],\n",
      "        [  101, 20898, 14569,  ...,     0,     0,     0],\n",
      "        [  101,   170,  2528,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 21748,   157,  ...,     0,     0,     0],\n",
      "        [  101,  3446,   188,  ...,     0,     0,     0],\n",
      "        [  101,   153,  6447,  ...,     0,     0,     0]])), ('token_type_ids', tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])), ('attention_mask', tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])), ('labels', tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]))])\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "sample = next(train_iter)\n",
    "print(sample.items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model, Optimizer Function, and Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will declare the model, the optimizer function used to optimize the model, and the loss function that is to be minimized as part of the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# configuration = BertConfig(hidden_dropout_prob=0.3, num_hidden_layers = 12, attention_probs_dropout_prob = 0.4)\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(bert_model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.to(device) # Transfer model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Training batch 1 last loss: 0.017877323925495146\n",
      "Training batch 2 last loss: 0.018201589584350586\n",
      "Training batch 3 last loss: 0.017984268069267274\n",
      "Training batch 4 last loss: 0.017446587979793548\n",
      "Training batch 5 last loss: 0.017330190539360045\n",
      "Training batch 6 last loss: 0.01730782687664032\n",
      "Training batch 7 last loss: 0.016722582280635834\n",
      "Training batch 8 last loss: 0.01714702248573303\n",
      "Training batch 9 last loss: 0.016864123940467834\n",
      "Training batch 10 last loss: 0.01759873330593109\n",
      "Training batch 11 last loss: 0.016695693135261536\n",
      "Training batch 12 last loss: 0.016329213976860046\n",
      "Training batch 13 last loss: 0.01644885540008545\n",
      "Training batch 14 last loss: 0.01563907265663147\n",
      "Training batch 15 last loss: 0.017045900225639343\n",
      "Training batch 16 last loss: 0.016291584074497222\n",
      "Training batch 17 last loss: 0.017157545685768126\n",
      "Training batch 18 last loss: 0.016683897376060484\n",
      "Training batch 19 last loss: 0.016242915391922\n",
      "Training batch 20 last loss: 0.015635231137275697\n",
      "Training batch 21 last loss: 0.016200116276741026\n",
      "Training batch 22 last loss: 0.015812113881111145\n",
      "Training batch 23 last loss: 0.015482737123966217\n",
      "Training batch 24 last loss: 0.015017691254615783\n",
      "Training batch 25 last loss: 0.01443357914686203\n",
      "Training batch 26 last loss: 0.01408749669790268\n",
      "Training batch 27 last loss: 0.013037240505218506\n",
      "Training batch 28 last loss: 0.013855388760566712\n",
      "Training batch 29 last loss: 0.016072753071784972\n",
      "Training batch 30 last loss: 0.01345897763967514\n",
      "Training batch 31 last loss: 0.015523044764995575\n",
      "Training batch 32 last loss: 0.01291184276342392\n",
      "Training batch 33 last loss: 0.012237106263637543\n",
      "Training batch 34 last loss: 0.01310444325208664\n",
      "Training batch 35 last loss: 0.013800950348377227\n",
      "Training batch 36 last loss: 0.013356238603591919\n",
      "Training batch 37 last loss: 0.01177889108657837\n",
      "Training batch 38 last loss: 0.009010159969329834\n",
      "Training batch 39 last loss: 0.013026520609855652\n",
      "Training batch 40 last loss: 0.014563527703285218\n",
      "Training batch 41 last loss: 0.011551515012979508\n",
      "Training batch 42 last loss: 0.011520642042160033\n",
      "Training batch 43 last loss: 0.011189696937799453\n",
      "Training batch 44 last loss: 0.011342096328735351\n",
      "Training batch 45 last loss: 0.011039631068706512\n",
      "Training batch 46 last loss: 0.010719474405050278\n",
      "Training batch 47 last loss: 0.010395066440105438\n",
      "Training batch 48 last loss: 0.009122466295957565\n",
      "Training batch 49 last loss: 0.01139487251639366\n",
      "Training batch 50 last loss: 0.00946451649069786\n",
      "Training batch 51 last loss: 0.01020633578300476\n",
      "Training batch 52 last loss: 0.010891969501972198\n",
      "Training batch 53 last loss: 0.010089346021413804\n",
      "Training batch 54 last loss: 0.010610978305339813\n",
      "Training batch 55 last loss: 0.008265069127082825\n",
      "Training batch 56 last loss: 0.009665838629007339\n",
      "Training batch 57 last loss: 0.0107752226293087\n",
      "Training batch 58 last loss: 0.011254574358463287\n",
      "Training batch 59 last loss: 0.007628875970840454\n",
      "Training batch 60 last loss: 0.008791761845350266\n",
      "Training batch 61 last loss: 0.009662080556154251\n",
      "Training batch 62 last loss: 0.009370948374271392\n",
      "Training batch 63 last loss: 0.010112659633159637\n",
      "Training batch 64 last loss: 0.007735691964626312\n",
      "Training batch 65 last loss: 0.00900493487715721\n",
      "Training batch 66 last loss: 0.009956159442663193\n",
      "Training batch 67 last loss: 0.007236758619546891\n",
      "Training batch 68 last loss: 0.006798059493303299\n",
      "Training batch 69 last loss: 0.010645218938589097\n",
      "Training batch 70 last loss: 0.00873095765709877\n",
      "Training batch 71 last loss: 0.0064450189471244815\n",
      "Training batch 72 last loss: 0.00663846880197525\n",
      "Training batch 73 last loss: 0.011138994991779328\n",
      "Training batch 74 last loss: 0.006110380217432976\n",
      "Training batch 75 last loss: 0.007259620726108551\n",
      "Training batch 76 last loss: 0.007123514264822006\n",
      "Training batch 77 last loss: 0.008084815740585328\n",
      "Training batch 78 last loss: 0.004847900941967964\n",
      "Training batch 79 last loss: 0.010394036769866943\n",
      "Training batch 80 last loss: 0.008938323706388474\n",
      "Training batch 81 last loss: 0.008024661242961884\n",
      "Training batch 82 last loss: 0.004625900834798813\n",
      "Training batch 83 last loss: 0.006696099042892456\n",
      "Training batch 84 last loss: 0.009306326508522034\n",
      "Training batch 85 last loss: 0.008513490110635758\n",
      "Training batch 86 last loss: 0.0061581235378980635\n",
      "Training batch 87 last loss: 0.005921459943056107\n",
      "Training batch 88 last loss: 0.005447926372289658\n",
      "Training batch 89 last loss: 0.008076777309179306\n",
      "Training batch 90 last loss: 0.009106004983186722\n",
      "Training batch 91 last loss: 0.002895013988018036\n",
      "Training batch 92 last loss: 0.0078147292137146\n",
      "Training batch 93 last loss: 0.012461338937282563\n",
      "\n",
      "Training epoch 1 loss:  0.012461338937282563\n",
      "Testing batch 1 loss: 0.01585824191570282\n",
      "Testing accuracy:  0.75\n",
      "Testing batch 2 loss: 0.0052086997777223585\n",
      "Testing accuracy:  0.85\n",
      "Testing batch 3 loss: 0.01018265038728714\n",
      "Testing accuracy:  0.8333333333333334\n",
      "Testing batch 4 loss: 0.008985050022602081\n",
      "Testing accuracy:  0.8375\n",
      "Testing batch 5 loss: 0.006374308466911316\n",
      "Testing accuracy:  0.85\n",
      "Testing batch 6 loss: 0.006745519489049912\n",
      "Testing accuracy:  0.8625\n",
      "Testing batch 7 loss: 0.004705204069614411\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 8 loss: 0.007811236381530762\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 9 loss: 0.008896802365779877\n",
      "Testing accuracy:  0.8694444444444445\n",
      "Testing batch 10 loss: 0.01020977646112442\n",
      "Testing accuracy:  0.8625\n",
      "Testing batch 11 loss: 0.00446755401790142\n",
      "Testing accuracy:  0.8727272727272727\n",
      "Testing batch 12 loss: 0.007939621061086654\n",
      "Testing accuracy:  0.8708333333333333\n",
      "Testing batch 13 loss: 0.006699518114328384\n",
      "Testing accuracy:  0.8730769230769231\n",
      "Testing batch 14 loss: 0.009047314524650574\n",
      "Testing accuracy:  0.8696428571428572\n",
      "Testing batch 15 loss: 0.004905600845813751\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 16 loss: 0.005753174424171448\n",
      "Testing accuracy:  0.8765625\n",
      "Testing batch 17 loss: 0.00792316123843193\n",
      "Testing accuracy:  0.8779411764705882\n",
      "Testing batch 18 loss: 0.007970534265041351\n",
      "Testing accuracy:  0.8763888888888889\n",
      "Testing batch 19 loss: 0.00807999074459076\n",
      "Testing accuracy:  0.8763157894736842\n",
      "Testing batch 20 loss: 0.010664649307727814\n",
      "Testing accuracy:  0.8725\n",
      "Testing batch 21 loss: 0.0049690306186676025\n",
      "Testing accuracy:  0.8738095238095238\n",
      "Testing batch 22 loss: 0.005848478153347969\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 23 loss: 0.0075662188231945034\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 24 loss: 0.0063665851950645445\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 25 loss: 0.008538269251585007\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 26 loss: 0.0065487615764141084\n",
      "Testing accuracy:  0.8759615384615385\n",
      "Testing batch 27 loss: 0.006527751684188843\n",
      "Testing accuracy:  0.8768518518518519\n",
      "Testing batch 28 loss: 0.004962102696299553\n",
      "Testing accuracy:  0.8803571428571428\n",
      "Testing batch 29 loss: 0.007389070093631744\n",
      "Testing accuracy:  0.8810344827586207\n",
      "Testing batch 30 loss: 0.008922161161899566\n",
      "Testing accuracy:  0.8791666666666667\n",
      "Testing batch 31 loss: 0.005063411593437195\n",
      "Testing accuracy:  0.8806451612903226\n",
      "Testing batch 32 loss: 0.01397794634103775\n",
      "Testing accuracy:  0.87734375\n",
      "Testing batch 33 loss: 0.004727838188409805\n",
      "Testing accuracy:  0.8795454545454545\n",
      "Testing batch 34 loss: 0.008196122944355011\n",
      "Testing accuracy:  0.8794117647058823\n",
      "Testing batch 35 loss: 0.007548456639051437\n",
      "Testing accuracy:  0.8792857142857143\n",
      "Testing batch 36 loss: 0.0074295707046985624\n",
      "Testing accuracy:  0.8784722222222222\n",
      "Testing batch 37 loss: 0.010312242805957795\n",
      "Testing accuracy:  0.8770270270270271\n",
      "Testing batch 38 loss: 0.00904095396399498\n",
      "Testing accuracy:  0.8769736842105263\n",
      "Testing batch 39 loss: 0.007747901231050491\n",
      "Testing accuracy:  0.8769230769230769\n",
      "Testing batch 40 loss: 0.004642947018146515\n",
      "Testing accuracy:  0.87125\n",
      "\n",
      "Testing epoch 1 last loss:  0.004642947018146515\n",
      "Epoch:  2\n",
      "Training batch 1 last loss: 0.00915297567844391\n",
      "Training batch 2 last loss: 0.004066542536020279\n",
      "Training batch 3 last loss: 0.0078023262321949005\n",
      "Training batch 4 last loss: 0.007049660384654999\n",
      "Training batch 5 last loss: 0.0069151662290096285\n",
      "Training batch 6 last loss: 0.003462878614664078\n",
      "Training batch 7 last loss: 0.003911446779966354\n",
      "Training batch 8 last loss: 0.005045140534639359\n",
      "Training batch 9 last loss: 0.005781716853380203\n",
      "Training batch 10 last loss: 0.004549285396933556\n",
      "Training batch 11 last loss: 0.005826576426625252\n",
      "Training batch 12 last loss: 0.004514990001916885\n",
      "Training batch 13 last loss: 0.004190496727824211\n",
      "Training batch 14 last loss: 0.004533981531858444\n",
      "Training batch 15 last loss: 0.006319661438465118\n",
      "Training batch 16 last loss: 0.005720018595457077\n",
      "Training batch 17 last loss: 0.0042597446590662\n",
      "Training batch 18 last loss: 0.007625659555196762\n",
      "Training batch 19 last loss: 0.003163203224539757\n",
      "Training batch 20 last loss: 0.008303109556436539\n",
      "Training batch 21 last loss: 0.005481638386845589\n",
      "Training batch 22 last loss: 0.003394632786512375\n",
      "Training batch 23 last loss: 0.007520756870508194\n",
      "Training batch 24 last loss: 0.004315692186355591\n",
      "Training batch 25 last loss: 0.007475455850362777\n",
      "Training batch 26 last loss: 0.004736150428652763\n",
      "Training batch 27 last loss: 0.005183656141161919\n",
      "Training batch 28 last loss: 0.005225419253110886\n",
      "Training batch 29 last loss: 0.0033069729804992678\n",
      "Training batch 30 last loss: 0.003397230803966522\n",
      "Training batch 31 last loss: 0.004932042956352234\n",
      "Training batch 32 last loss: 0.009215138852596283\n",
      "Training batch 33 last loss: 0.007425332069396972\n",
      "Training batch 34 last loss: 0.0032572317868471146\n",
      "Training batch 35 last loss: 0.00929269939661026\n",
      "Training batch 36 last loss: 0.0022549329325556753\n",
      "Training batch 37 last loss: 0.012078545242547988\n",
      "Training batch 38 last loss: 0.004168857634067535\n",
      "Training batch 39 last loss: 0.0060845594853162766\n",
      "Training batch 40 last loss: 0.006750611960887909\n",
      "Training batch 41 last loss: 0.0076106123626232145\n",
      "Training batch 42 last loss: 0.003917816653847694\n",
      "Training batch 43 last loss: 0.00983513742685318\n",
      "Training batch 44 last loss: 0.006619273126125336\n",
      "Training batch 45 last loss: 0.008576484769582749\n",
      "Training batch 46 last loss: 0.008813775330781936\n",
      "Training batch 47 last loss: 0.005040623992681503\n",
      "Training batch 48 last loss: 0.004594623297452927\n",
      "Training batch 49 last loss: 0.004133591800928116\n",
      "Training batch 50 last loss: 0.008790752291679383\n",
      "Training batch 51 last loss: 0.005184616521000862\n",
      "Training batch 52 last loss: 0.006516213715076447\n",
      "Training batch 53 last loss: 0.0019103659316897393\n",
      "Training batch 54 last loss: 0.006365039944648742\n",
      "Training batch 55 last loss: 0.0037016667425632476\n",
      "Training batch 56 last loss: 0.005144559219479561\n",
      "Training batch 57 last loss: 0.003666911646723747\n",
      "Training batch 58 last loss: 0.00349089577794075\n",
      "Training batch 59 last loss: 0.007299214601516724\n",
      "Training batch 60 last loss: 0.003363410010933876\n",
      "Training batch 61 last loss: 0.004096458107233048\n",
      "Training batch 62 last loss: 0.003472895920276642\n",
      "Training batch 63 last loss: 0.006273946911096573\n",
      "Training batch 64 last loss: 0.006939665973186493\n",
      "Training batch 65 last loss: 0.004729091748595237\n",
      "Training batch 66 last loss: 0.0025571662932634353\n",
      "Training batch 67 last loss: 0.0062045354396104814\n",
      "Training batch 68 last loss: 0.004199638217687607\n",
      "Training batch 69 last loss: 0.006189298629760742\n",
      "Training batch 70 last loss: 0.0028605151921510695\n",
      "Training batch 71 last loss: 0.005894523113965988\n",
      "Training batch 72 last loss: 0.006612152606248855\n",
      "Training batch 73 last loss: 0.0043574035167694095\n",
      "Training batch 74 last loss: 0.0035157151520252227\n",
      "Training batch 75 last loss: 0.004341426491737366\n",
      "Training batch 76 last loss: 0.003946616128087044\n",
      "Training batch 77 last loss: 0.005474318191409111\n",
      "Training batch 78 last loss: 0.004185188934206963\n",
      "Training batch 79 last loss: 0.003981329500675201\n",
      "Training batch 80 last loss: 0.003221113234758377\n",
      "Training batch 81 last loss: 0.0021883929148316383\n",
      "Training batch 82 last loss: 0.006165019422769547\n",
      "Training batch 83 last loss: 0.0023352313786745072\n",
      "Training batch 84 last loss: 0.0033866040408611297\n",
      "Training batch 85 last loss: 0.0030175575986504553\n",
      "Training batch 86 last loss: 0.010076282918453217\n",
      "Training batch 87 last loss: 0.002556160092353821\n",
      "Training batch 88 last loss: 0.008910459280014039\n",
      "Training batch 89 last loss: 0.0026248713955283167\n",
      "Training batch 90 last loss: 0.002420814894139767\n",
      "Training batch 91 last loss: 0.005485261231660843\n",
      "Training batch 92 last loss: 0.005918627232313156\n",
      "Training batch 93 last loss: 0.0010524814948439597\n",
      "\n",
      "Training epoch 2 loss:  0.0010524814948439597\n",
      "Testing batch 1 loss: 0.004112473875284195\n",
      "Testing accuracy:  0.95\n",
      "Testing batch 2 loss: 0.008874766528606415\n",
      "Testing accuracy:  0.8875\n",
      "Testing batch 3 loss: 0.01022961139678955\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 4 loss: 0.00413106307387352\n",
      "Testing accuracy:  0.89375\n",
      "Testing batch 5 loss: 0.00955437570810318\n",
      "Testing accuracy:  0.885\n",
      "Testing batch 6 loss: 0.0023925652727484702\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 7 loss: 0.002416263520717621\n",
      "Testing accuracy:  0.9142857142857143\n",
      "Testing batch 8 loss: 0.005810918286442757\n",
      "Testing accuracy:  0.915625\n",
      "Testing batch 9 loss: 0.009582959115505219\n",
      "Testing accuracy:  0.9138888888888889\n",
      "Testing batch 10 loss: 0.005694002658128738\n",
      "Testing accuracy:  0.9125\n",
      "Testing batch 11 loss: 0.007421799749135971\n",
      "Testing accuracy:  0.9113636363636364\n",
      "Testing batch 12 loss: 0.006565511971712112\n",
      "Testing accuracy:  0.9125\n",
      "Testing batch 13 loss: 0.005730912089347839\n",
      "Testing accuracy:  0.9134615384615384\n",
      "Testing batch 14 loss: 0.009251461178064347\n",
      "Testing accuracy:  0.9089285714285714\n",
      "Testing batch 15 loss: 0.009403096139431\n",
      "Testing accuracy:  0.9016666666666666\n",
      "Testing batch 16 loss: 0.006517843157052994\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 17 loss: 0.00531904473900795\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 18 loss: 0.005679550766944885\n",
      "Testing accuracy:  0.9013888888888889\n",
      "Testing batch 19 loss: 0.006530559062957764\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 20 loss: 0.007135605067014694\n",
      "Testing accuracy:  0.89875\n",
      "Testing batch 21 loss: 0.012485406547784805\n",
      "Testing accuracy:  0.8928571428571429\n",
      "Testing batch 22 loss: 0.009580530971288682\n",
      "Testing accuracy:  0.8920454545454546\n",
      "Testing batch 23 loss: 0.004240986704826355\n",
      "Testing accuracy:  0.8945652173913043\n",
      "Testing batch 24 loss: 0.002477181889116764\n",
      "Testing accuracy:  0.8979166666666667\n",
      "Testing batch 25 loss: 0.006173243373632431\n",
      "Testing accuracy:  0.897\n",
      "Testing batch 26 loss: 0.00439462698996067\n",
      "Testing accuracy:  0.8980769230769231\n",
      "Testing batch 27 loss: 0.014240296185016632\n",
      "Testing accuracy:  0.8944444444444445\n",
      "Testing batch 28 loss: 0.009094709903001786\n",
      "Testing accuracy:  0.89375\n",
      "Testing batch 29 loss: 0.00826837420463562\n",
      "Testing accuracy:  0.8931034482758621\n",
      "Testing batch 30 loss: 0.008926121145486831\n",
      "Testing accuracy:  0.8908333333333334\n",
      "Testing batch 31 loss: 0.011872716993093491\n",
      "Testing accuracy:  0.8879032258064516\n",
      "Testing batch 32 loss: 0.0023816773667931558\n",
      "Testing accuracy:  0.89140625\n",
      "Testing batch 33 loss: 0.007349054515361786\n",
      "Testing accuracy:  0.8901515151515151\n",
      "Testing batch 34 loss: 0.004504409059882164\n",
      "Testing accuracy:  0.8919117647058824\n",
      "Testing batch 35 loss: 0.00562138445675373\n",
      "Testing accuracy:  0.8928571428571429\n",
      "Testing batch 36 loss: 0.004573020339012146\n",
      "Testing accuracy:  0.8944444444444445\n",
      "Testing batch 37 loss: 0.004645514115691185\n",
      "Testing accuracy:  0.8952702702702703\n",
      "Testing batch 38 loss: 0.005367226153612137\n",
      "Testing accuracy:  0.8953947368421052\n",
      "Testing batch 39 loss: 0.008397282660007476\n",
      "Testing accuracy:  0.8942307692307693\n",
      "Testing batch 40 loss: 0.008642420172691345\n",
      "Testing accuracy:  0.886875\n",
      "\n",
      "Testing epoch 2 last loss:  0.008642420172691345\n",
      "Epoch:  3\n",
      "Training batch 1 last loss: 0.00169809777289629\n",
      "Training batch 2 last loss: 0.0014419632963836194\n",
      "Training batch 3 last loss: 0.00511275939643383\n",
      "Training batch 4 last loss: 0.0036382026970386504\n",
      "Training batch 5 last loss: 0.0072828978300094604\n",
      "Training batch 6 last loss: 0.0043497271835803986\n",
      "Training batch 7 last loss: 0.0023961110040545464\n",
      "Training batch 8 last loss: 0.0053650371730327604\n",
      "Training batch 9 last loss: 0.004778841882944107\n",
      "Training batch 10 last loss: 0.0023342272266745566\n",
      "Training batch 11 last loss: 0.003073231875896454\n",
      "Training batch 12 last loss: 0.002265026979148388\n",
      "Training batch 13 last loss: 0.00310117956250906\n",
      "Training batch 14 last loss: 0.00263627115637064\n",
      "Training batch 15 last loss: 0.002659052051603794\n",
      "Training batch 16 last loss: 0.0011718038469552994\n",
      "Training batch 17 last loss: 0.0012415394186973572\n",
      "Training batch 18 last loss: 0.004005887359380722\n",
      "Training batch 19 last loss: 0.008240558952093125\n",
      "Training batch 20 last loss: 0.0019518744200468063\n",
      "Training batch 21 last loss: 0.005695086717605591\n",
      "Training batch 22 last loss: 0.0013868717476725578\n",
      "Training batch 23 last loss: 0.008581610023975372\n",
      "Training batch 24 last loss: 0.004413863271474838\n",
      "Training batch 25 last loss: 0.001808542385697365\n",
      "Training batch 26 last loss: 0.002587122283875942\n",
      "Training batch 27 last loss: 0.001782829686999321\n",
      "Training batch 28 last loss: 0.004610348865389824\n",
      "Training batch 29 last loss: 0.009888339787721634\n",
      "Training batch 30 last loss: 0.0017092300578951836\n",
      "Training batch 31 last loss: 0.003377503901720047\n",
      "Training batch 32 last loss: 0.0015266204252839088\n",
      "Training batch 33 last loss: 0.00189290102571249\n",
      "Training batch 34 last loss: 0.0050408847630023955\n",
      "Training batch 35 last loss: 0.0029494766145944594\n",
      "Training batch 36 last loss: 0.0017430810257792473\n",
      "Training batch 37 last loss: 0.0041092030704021456\n",
      "Training batch 38 last loss: 0.002289334125816822\n",
      "Training batch 39 last loss: 0.002304822579026222\n",
      "Training batch 40 last loss: 0.00829537957906723\n",
      "Training batch 41 last loss: 0.00356704480946064\n",
      "Training batch 42 last loss: 0.003401772677898407\n",
      "Training batch 43 last loss: 0.0020544711500406266\n",
      "Training batch 44 last loss: 0.002442975901067257\n",
      "Training batch 45 last loss: 0.0023977568373084067\n",
      "Training batch 46 last loss: 0.002695864997804165\n",
      "Training batch 47 last loss: 0.0008055035956203938\n",
      "Training batch 48 last loss: 0.003938591107726097\n",
      "Training batch 49 last loss: 0.004087503254413605\n",
      "Training batch 50 last loss: 0.001710849441587925\n",
      "Training batch 51 last loss: 0.006723266094923019\n",
      "Training batch 52 last loss: 0.005051422119140625\n",
      "Training batch 53 last loss: 0.00360976941883564\n",
      "Training batch 54 last loss: 0.001479772012680769\n",
      "Training batch 55 last loss: 0.0019798403605818748\n",
      "Training batch 56 last loss: 0.002931954339146614\n",
      "Training batch 57 last loss: 0.002672533318400383\n",
      "Training batch 58 last loss: 0.001439038198441267\n",
      "Training batch 59 last loss: 0.006790515035390854\n",
      "Training batch 60 last loss: 0.002830127440392971\n",
      "Training batch 61 last loss: 0.007204488664865494\n",
      "Training batch 62 last loss: 0.008021807670593262\n",
      "Training batch 63 last loss: 0.0026071425527334213\n",
      "Training batch 64 last loss: 0.003917224332690239\n",
      "Training batch 65 last loss: 0.004133297875523567\n",
      "Training batch 66 last loss: 0.0029364565387368204\n",
      "Training batch 67 last loss: 0.001121982280164957\n",
      "Training batch 68 last loss: 0.004292355850338936\n",
      "Training batch 69 last loss: 0.0025876391679048537\n",
      "Training batch 70 last loss: 0.002416371926665306\n",
      "Training batch 71 last loss: 0.005439593270421028\n",
      "Training batch 72 last loss: 0.0034717120230197906\n",
      "Training batch 73 last loss: 0.004175978153944016\n",
      "Training batch 74 last loss: 0.004352762177586555\n",
      "Training batch 75 last loss: 0.003068869933485985\n",
      "Training batch 76 last loss: 0.006370745599269867\n",
      "Training batch 77 last loss: 0.0009195700287818909\n",
      "Training batch 78 last loss: 0.005767177417874336\n",
      "Training batch 79 last loss: 0.004521096125245094\n",
      "Training batch 80 last loss: 0.0013257515616714954\n",
      "Training batch 81 last loss: 0.0052900612354278564\n",
      "Training batch 82 last loss: 0.002602032385766506\n",
      "Training batch 83 last loss: 0.0023231614381074906\n",
      "Training batch 84 last loss: 0.0022077552974224092\n",
      "Training batch 85 last loss: 0.0013865170069038868\n",
      "Training batch 86 last loss: 0.0029681434854865072\n",
      "Training batch 87 last loss: 0.006911034882068634\n",
      "Training batch 88 last loss: 0.002518906816840172\n",
      "Training batch 89 last loss: 0.004361517354846\n",
      "Training batch 90 last loss: 0.004281462728977203\n",
      "Training batch 91 last loss: 0.004156740009784698\n",
      "Training batch 92 last loss: 0.0015486362390220165\n",
      "Training batch 93 last loss: 0.0057277105748653415\n",
      "\n",
      "Training epoch 3 loss:  0.0057277105748653415\n",
      "Testing batch 1 loss: 0.008951441198587418\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 2 loss: 0.009787239879369736\n",
      "Testing accuracy:  0.8875\n",
      "Testing batch 3 loss: 0.00624505802989006\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 4 loss: 0.005842248722910881\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 5 loss: 0.004707973077893257\n",
      "Testing accuracy:  0.91\n",
      "Testing batch 6 loss: 0.005627087876200676\n",
      "Testing accuracy:  0.9125\n",
      "Testing batch 7 loss: 0.005836928635835648\n",
      "Testing accuracy:  0.9142857142857143\n",
      "Testing batch 8 loss: 0.005490213632583618\n",
      "Testing accuracy:  0.9125\n",
      "Testing batch 9 loss: 0.0039512161165475845\n",
      "Testing accuracy:  0.9138888888888889\n",
      "Testing batch 10 loss: 0.008965481817722321\n",
      "Testing accuracy:  0.9075\n",
      "Testing batch 11 loss: 0.009821511805057526\n",
      "Testing accuracy:  0.9045454545454545\n",
      "Testing batch 12 loss: 0.006607277691364289\n",
      "Testing accuracy:  0.9041666666666667\n",
      "Testing batch 13 loss: 0.002477671392261982\n",
      "Testing accuracy:  0.9076923076923077\n",
      "Testing batch 14 loss: 0.009371583163738251\n",
      "Testing accuracy:  0.9035714285714286\n",
      "Testing batch 15 loss: 0.008637503534555436\n",
      "Testing accuracy:  0.905\n",
      "Testing batch 16 loss: 0.005468574911355972\n",
      "Testing accuracy:  0.903125\n",
      "Testing batch 17 loss: 0.006102791428565979\n",
      "Testing accuracy:  0.9029411764705882\n",
      "Testing batch 18 loss: 0.007558475434780121\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 19 loss: 0.012958064675331116\n",
      "Testing accuracy:  0.8973684210526316\n",
      "Testing batch 20 loss: 0.01085321456193924\n",
      "Testing accuracy:  0.89625\n",
      "Testing batch 21 loss: 0.007370482385158539\n",
      "Testing accuracy:  0.8976190476190476\n",
      "Testing batch 22 loss: 0.0041195530444383625\n",
      "Testing accuracy:  0.8988636363636363\n",
      "Testing batch 23 loss: 0.006922971457242966\n",
      "Testing accuracy:  0.8989130434782608\n",
      "Testing batch 24 loss: 0.003214699774980545\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 25 loss: 0.003966215997934341\n",
      "Testing accuracy:  0.902\n",
      "Testing batch 26 loss: 0.00924033671617508\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 27 loss: 0.007817183434963227\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 28 loss: 0.0060872603207826614\n",
      "Testing accuracy:  0.9008928571428572\n",
      "Testing batch 29 loss: 0.009765143692493438\n",
      "Testing accuracy:  0.8991379310344828\n",
      "Testing batch 30 loss: 0.005493860691785812\n",
      "Testing accuracy:  0.9\n",
      "Testing batch 31 loss: 0.006152191013097763\n",
      "Testing accuracy:  0.8991935483870968\n",
      "Testing batch 32 loss: 0.011054082959890365\n",
      "Testing accuracy:  0.89765625\n",
      "Testing batch 33 loss: 0.008157428354024887\n",
      "Testing accuracy:  0.8962121212121212\n",
      "Testing batch 34 loss: 0.006213521957397461\n",
      "Testing accuracy:  0.8955882352941177\n",
      "Testing batch 35 loss: 0.010533289611339569\n",
      "Testing accuracy:  0.8935714285714286\n",
      "Testing batch 36 loss: 0.006749390065670014\n",
      "Testing accuracy:  0.89375\n",
      "Testing batch 37 loss: 0.005126332119107247\n",
      "Testing accuracy:  0.893918918918919\n",
      "Testing batch 38 loss: 0.0058537166565656665\n",
      "Testing accuracy:  0.8947368421052632\n",
      "Testing batch 39 loss: 0.00500074215233326\n",
      "Testing accuracy:  0.8955128205128206\n",
      "Testing batch 40 loss: 0.010120496898889542\n",
      "Testing accuracy:  0.8875\n",
      "\n",
      "Testing epoch 3 last loss:  0.010120496898889542\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \",(epoch + 1))\n",
    "    # TRAINING BLOCK STARTS\n",
    "    bert_model.train()\n",
    "    for i,batch in enumerate(train_loader):    \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Setting the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passing the data to the model\n",
    "        outputs = bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        # The logits will be used for measuring the loss\n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, batch['labels'])\n",
    "\n",
    "        # Calculating the gradient for the loss function\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizing the parameters of the bert model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating the running loss for logging purposes\n",
    "        train_batch_loss = loss.item()\n",
    "        train_last_loss = train_batch_loss / batch_size\n",
    "\n",
    "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "    # Logging epoch-wise training loss\n",
    "    print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "    # TRAINING BLOCK ENDS \n",
    "\n",
    "    # TESTING BLOCK STARTS\n",
    "    bert_model.eval()\n",
    "    correct = 0\n",
    "    test_pred = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # We don't need gradients for testing\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        # Logits act as predictions\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculating total batch loss using the logits and labels\n",
    "        loss = loss_fn(logits, batch['labels'])\n",
    "        test_batch_loss = loss.item()\n",
    "        \n",
    "        # Calculating the mean batch loss\n",
    "        test_last_loss = test_batch_loss / batch_size\n",
    "        print('Testing batch {} loss: {}'.format(i + 1, test_last_loss))\n",
    "        \n",
    "        # Comparing the predicted target with the labels in the batch\n",
    "        correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "        print(\"Testing accuracy: \",correct/((i + 1) * batch_size))\n",
    "    \n",
    "    print(f\"\\nTesting epoch {epoch + 1} last loss: \",test_last_loss)\n",
    "    # TESTING BLOCK ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.8947700063011972\n"
     ]
    }
   ],
   "source": [
    "print('Validation accuracy: ',correct/len(test_X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model.state_dict(), \"./model/model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2</td>\n",
       "      <td>Why is  explained in the video take a look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3</td>\n",
       "      <td>Ed Davey fasting for Ramadan No contest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4</td>\n",
       "      <td>Is Doja Cat good or do you just miss Nicki Minaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_8</td>\n",
       "      <td>How Boris Johnson s cheery wounded in action p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_9</td>\n",
       "      <td>Man it s terrible Not even a reason to get on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>test_2932</td>\n",
       "      <td>Fageeru meehaa geyga Bandah PUBLIC fundS amp G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>test_2934</td>\n",
       "      <td>DFFN Diffusion Pharmaceuticals Announces Pre I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>test_2936</td>\n",
       "      <td>I want to wish the Muslim members of Congress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>test_2937</td>\n",
       "      <td>You mean you don t believe there is a conspira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>test_2940</td>\n",
       "      <td>Rajavi We call on the United Nations and the S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1962 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               text\n",
       "0        test_2         Why is  explained in the video take a look\n",
       "1        test_3            Ed Davey fasting for Ramadan No contest\n",
       "2        test_4   Is Doja Cat good or do you just miss Nicki Minaj\n",
       "3        test_8  How Boris Johnson s cheery wounded in action p...\n",
       "4        test_9  Man it s terrible Not even a reason to get on ...\n",
       "...         ...                                                ...\n",
       "1957  test_2932  Fageeru meehaa geyga Bandah PUBLIC fundS amp G...\n",
       "1958  test_2934  DFFN Diffusion Pharmaceuticals Announces Pre I...\n",
       "1959  test_2936  I want to wish the Muslim members of Congress ...\n",
       "1960  test_2937  You mean you don t believe there is a conspira...\n",
       "1961  test_2940  Rajavi We call on the United Nations and the S...\n",
       "\n",
       "[1962 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tokens = tokenizer(list(test_data['text']), padding = True, truncation=True)\n",
    "len(test_data_tokens['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Dataset class for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    def __init__(self):\n",
    "            self.text_data = test_data['text']\n",
    "            self.tokens = test_data_tokens\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        return sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring an object for test dataset and test data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dataset = TestData()\n",
    "test_data_loader = DataLoader(test_data_dataset, batch_size=1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved (trained) model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32398/368546927.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(\"./model/model.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load(\"./model/model.pt\")\n",
    "bert_model.load_state_dict(weights)\n",
    "bert_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.eval()\n",
    "result = []\n",
    "for i, batch in enumerate(test_data_loader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    result.extend(list(torch.nn.functional.softmax(logits, dim = 1).type(torch.float)))\n",
    "result = [i[1].item() for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df['ID'] = test_data['ID']\n",
    "result_df['target'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"./submission1.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
