{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Quelques précisions :\n",
        "- Dans un premier temps le modèle ne s'entraînera pas à prédire seulement les métiers présent dans la phrase, mais tous les mots. Pour prédire seulement les métiers il faudrait soit des phrases où le métier est toujours placé à la même position, comme ca on peut demander au modèle de mask input[index] mais ca contraint énormément le type de phrase. Soit on cherche un algo automatique qui déduit les tokens en rapport avec les métiers (mais ca prend du temps à chercher/implémenter donc on va commencer sans). Sinon il faudrait avoir une liste de métier et on demande à les masquer mais ca contraint le type de métier parce qu'on ne pensera pas à tout. Par exemple un LLM avait généré comme phrase d'entrée (elle est capitaine de l'équipe de basket professionelle ect) Quels tokens masquer ici?\n",
        "- Pour l'instant la fonction de coût utilisée est une MSE entre output d'un mot versus la distribution de probabilité qu'on voudrait avoir et qui correspond à la moyenne des outputs pour phrase homme et phrase femme. A cela on ajoute un terme de régularisation entre logits 1 et logits 2 pour les contraindre à être à peu près similaire. (très fortement sujet à débat)"
      ],
      "metadata": {
        "id": "n8YyZpuzTl88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi9_NHn95K4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomTrainer(Trainer):\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
        "#         outputs_1 = model(input_ids=inputs[\"input_ids_1\"], attention_mask=inputs[\"attention_mask_1\"])\n",
        "#         logits1 = outputs_1.logits\n",
        "\n",
        "#         outputs_2 = model(input_ids=inputs[\"input_ids_2\"], attention_mask=inputs[\"attention_mask_2\"])\n",
        "#         logits2 = outputs_2.logits\n",
        "\n",
        "#         # loss1 = F.cross_entropy(logits1.view(-1, logits1.size(-1)), logits2.size(-1))  # Peut être qu'il vaudrait mieux faire la mean squared error ?\n",
        "#         # loss2 = F.cross_entropy(logits2.view(-1, logits2.size(-1)), logits1.size(-1))\n",
        "\n",
        "#         loss1.backward(retain_graph=True)\n",
        "#         loss2.backward()\n",
        "\n",
        "#         return (combined_loss, outputs_1) if return_outputs else combined_loss"
      ],
      "metadata": {
        "id": "c76_-NYM5amw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UARHHNfJQZYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataCollector(DataCollatorForLanguageModeling) :\n",
        "  def__init__(self):\n",
        "    super.__init__()\n",
        "    self.ground_truth = precompute_loss(model, inputs)\n",
        "\n",
        "  def torch_call(features): # On override torch call parce que sinon la méthode renvoit un dictionnaire avec les clés {input_ids, attention_mask, label}\n",
        "      return {\n",
        "        \"input_ids_1\": torch.stack([f[\"input_ids_1\"] for f in features]),\n",
        "        \"attention_mask_1\": torch.stack([f[\"attention_mask_1\"] for f in features]),\n",
        "        \"input_ids_2\": torch.stack([f[\"input_ids_2\"] for f in features]),\n",
        "        \"attention_mask_2\": torch.stack([f[\"attention_mask_2\"] for f in features]),\n",
        "        \"ground_truth\" : self.ground_truth # si je n'arrive pas à accéder à ground_truth depuis cette method, soit override __init__ de DataCollator soit passer et en faire un attribu\n",
        "        # ground_truth en argument torch_call\n",
        "      }"
      ],
      "metadata": {
        "id": "wYqBic8tMmpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not hasaatr(self.ground_truth):\n",
        "  self.ground_truth = (logits1 + logits2)/ 2\n",
        "  # Si logits1 et logits2 sont des tensors de taille (batch_size, vocab_size)\n",
        "  # Ground truth est aussi un tensor de taille (batch_size, sequence_lenght, vocab_size)\n",
        "  # on ne peut pas app\n",
        "  # Pour les garder il faudrait les placer dans labels, mais les labels n'auront pas la forme qu'ils sont sencé avoir\n",
        "\n",
        "def precompute_labels(model, inputs) :\n",
        "  # return a dictionnary input with one keywords added the ground_truth\n",
        "  input_1 = inputs[\"inputs_ids_1\"]\n",
        "  input_2 = inputs[\"inputs_ids_2\"]\n",
        "  logits1 = model(input_1) # vérifier que model(input_1) ne renvoie que l'output et rien d'autre sinon ca va créer une erreur\n",
        "  logits2 = model(input_2) # vérifier qu'on appelle bien forward de cet manière et qu'on peut lui passer en input un tensor au lieu d'un dictionnaire\n",
        "\n",
        "  assert logits1.shape == (batch_size, sequence_length vocab_size)\n",
        "  ground_truth = (logits1 + logits) /2\n",
        "  # inputs['ground_truth'] = ground_truth\n",
        "  # return inputs\n",
        "  return ground_truth\n",
        "\n",
        "ground_truth = precompute_labels(model, inputs)\n",
        "\n",
        "  # On a applique un forward pass sur tout le dataset et on met dans le dictionnaire des inputs mais pas sous le keyword label\n",
        "  # Limite : peut être très couteux computationnellemnt.\n",
        "  # La fonction de coût sera la MSE pour que ca soit plus simple même si c'est peut être pas le mieux."
      ],
      "metadata": {
        "id": "16sLsryuzsHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Biais_trainer(Trainer):\n",
        "  def __init__(self):\n",
        "    super.__init__()\n",
        "    self.custom_loss = nn.MSELoss()\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "    input_1 = inputs[\"inputs_ids_1\"]\n",
        "    input_2 = inputs[\"inputs_ids_2\"]\n",
        "    ground_truth = inputs[\"ground_truth\"]\n",
        "    logits1 = model(input_1) # vérifier que model(input_1) ne renvoie que l'output et rien d'autre sinon ca va créer une erreur (voir avec la doc de Bert)\n",
        "    logits2 = model(input_2)\n",
        "    combined_loss = self.custom_loss(logits1, ground_truth) + 0.5* self.custom_loss(logits1, logits2) # le deuxième terme est un terme de similarity constraint\n",
        "    return (combined_loss, logits_1) if return_outputs else combined_loss"
      ],
      "metadata": {
        "id": "pLba7h7QbEPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "custom_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"train.txt\",\n",
        "    block_size=128 # a changer pour qu'on puisse décider à chaque changement de ligne de prendre une autre phrase qui n'a aucun lien.\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-mlm-finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=custom_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "W1Eti3TmeUie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}