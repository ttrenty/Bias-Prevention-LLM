{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Transformers models in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 (Generative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\">Getting model gpt2\")\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\">Getting tokenizer for gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # Will yield a normal warning\n",
    "\n",
    "prompt = \"Large Langage Models are sometimes biased models.\"\n",
    "print(\">Defined the prompt:\", prompt)\n",
    "\n",
    "print(\">Defining tokenizer\")\n",
    "tokenizer_res = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = tokenizer_res.input_ids\n",
    "\n",
    "print(\">Generating new tokens\")\n",
    "gen_tokens = gpt2.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "print(\">Converting generated tokens to text\")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "print(\">Text generated:\\n\", gen_text)\n",
    "\n",
    "print(\">END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Descriptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input text\n",
    "input_text1 = \"Hello, how are you doing today?\"\n",
    "input_text2 = \"Hello, you're doing well today\"\n",
    "input_text3 = \"LLMs are sometimes very biased models.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs1 = tokenizer(input_text1, return_tensors='pt') #, padding='max_length', truncation=True, max_length=10)\n",
    "inputs2 = tokenizer(input_text2, return_tensors='pt') #, padding='max_length', truncation=True, max_length=10)\n",
    "inputs3 = tokenizer(input_text3, return_tensors='pt') #, padding='max_length', truncation=True, max_length=10)\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs1 = bert(**inputs1)\n",
    "outputs2 = bert(**inputs2)\n",
    "outputs3 = bert(**inputs3)\n",
    "\n",
    "# Extract activations from the last hidden state\n",
    "last_hidden_state1 = outputs1.last_hidden_state\n",
    "last_hidden_state2 = outputs2.last_hidden_state\n",
    "last_hidden_state3 = outputs3.last_hidden_state\n",
    "\n",
    "# Print the shape of the activations (batch_size, sequence_length, hidden_size)\n",
    "print(f\"Text 1: {input_text1}\")\n",
    "print(f\"Text 2: {input_text2}\")\n",
    "print(f\"Text 3: {input_text3}\")\n",
    "print(\"\")\n",
    "print(f\"Tokens for 1: {inputs1.input_ids}\")\n",
    "print(f\"Tokens for 2: {inputs2.input_ids}\")\n",
    "print(f\"Tokens for 3: {inputs3.input_ids}\")\n",
    "print(\"\")\n",
    "print(f\"Shape of bert output: {last_hidden_state1.shape}\")\n",
    "print(f\"Shape of output: {last_hidden_state2.shape}\")\n",
    "print(f\"Shape of output: {last_hidden_state3.shape}\")\n",
    "print(\"\")\n",
    "print(f\"Norm between example 1 and 2: {torch.norm(last_hidden_state1 - last_hidden_state2)}\")\n",
    "print(f\"Norm between example 1 and 3: {torch.norm(last_hidden_state1 - last_hidden_state3)}\")\n",
    "print(f\"Norm between example 2 and 3: {torch.norm(last_hidden_state2 - last_hidden_state3)}\")\n",
    "\n",
    "# # Optionally, print the activations (this could be a large tensor)\n",
    "# print(last_hidden_state1)\n",
    "# print(last_hidden_state2)\n",
    "# print(last_hidden_state3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a classifier with an MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from torch import nn\n",
    "\n",
    "class BERTClassification(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(BERTClassification, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        self.bert_drop = nn.Dropout(0.4)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, pooledOut = self.bert(ids, attention_mask = mask,\n",
    "                                token_type_ids=token_type_ids)\n",
    "        bertOut = self.bert_drop(pooledOut)\n",
    "        output = self.out(bertOut)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
