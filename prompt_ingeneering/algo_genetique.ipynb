{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_request(prompt) : \n",
    "\n",
    "    url = 'http://localhost:1234/v1/chat/completions'\n",
    "    # url = \"https://api.hyperbolic.xyz/v1/chat/completions\"\n",
    "\n",
    "    input_text = prompt\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": input_text}],\n",
    "        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        # \"model\": \"gpt-3.5-turbo\"  # Specify the model (if needed)\n",
    "    }\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',  \n",
    "        'Accept': 'application/json',        \n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJzeWxhZm9udEBlbnNjLmZyIiwiaWF0IjoxNzM2NDM5NjA0fQ.bulegzRLNecWwQyNS9Tdtjf89ftPrgy7KXAA7og3arA\"\n",
    "}\n",
    "    \"\"\"\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json() \n",
    "        model_response = response_data.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "        return model_response\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_file(filename, new_data):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Creating a new one.\")\n",
    "        data = []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"File is empty or invalid. Initializing with an empty list.\")\n",
    "        data = []\n",
    "    data.append(new_data)\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_history_from_prompt(filename, task_prompts, story_prompts):\n",
    "    specific_instruct = 'Write around 350 words'\n",
    "    for z, task_prompt in enumerate(task_prompts) :\n",
    "        list_story_for_one_prompt = []\n",
    "        for story_prompt in story_prompts:\n",
    "            list_stories_for_one_story_prompt =[]\n",
    "            for i in range(10):\n",
    "                intput_prompt = story_prompt+\"\\n\"+ task_prompt+ \"\\n\"+specific_instruct\n",
    "                # print(intput_prompt)\n",
    "                story_returned=\"\"\n",
    "                while \"STORY\" not in story_returned and \"story:\" not in  story_returned and \"STORY:\" not in  story_returned :\n",
    "                    story_returned = LLM_request(intput_prompt)\n",
    "                    #print(story_returned.split())\n",
    "                list_stories_for_one_story_prompt.append(story_returned)\n",
    "                # print(\"successfully added story\")\n",
    "            print(\"Go to next story\")\n",
    "            list_story_for_one_prompt.append(list_stories_for_one_story_prompt)\n",
    "        update_json_file(filename, list_story_for_one_prompt)\n",
    "        print(\"Passing to next prompts :\", z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def First_generation(filename):\n",
    "    with open(\"prompts_2.json\", \"r\") as file:\n",
    "        initial_prompt = json.load(file)\n",
    "    with open(\"prompt_for_biaised_story.json\", \"r\") as file:\n",
    "        roles = json.load(file)\n",
    "    generate_history_from_prompt(filename, initial_prompt, roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to next story\n",
      "Go to next story\n",
      "Go to next story\n",
      "File not found. Creating a new one.\n",
      "Passing to next prompts : 0\n",
      "Go to next story\n",
      "Go to next story\n",
      "Go to next story\n",
      "Passing to next prompts : 1\n",
      "Go to next story\n",
      "Go to next story\n",
      "Go to next story\n",
      "Passing to next prompts : 2\n",
      "Go to next story\n",
      "Go to next story\n",
      "Go to next story\n",
      "Passing to next prompts : 3\n",
      "Go to next story\n",
      "Go to next story\n",
      "Go to next story\n",
      "Passing to next prompts : 4\n",
      "Go to next story\n",
      "Go to next story\n",
      "Go to next story\n",
      "Passing to next prompts : 5\n",
      "Go to next story\n",
      "Go to next story\n",
      "Go to next story\n",
      "Passing to next prompts : 6\n",
      "Go to next story\n",
      "Go to next story\n",
      "Go to next story\n",
      "Passing to next prompts : 7\n"
     ]
    }
   ],
   "source": [
    "file_for_saving_story = \"first_generation_story_2.json\"\n",
    "First_generation(file_for_saving_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fitness_score(filename_story, filename_notes) :\n",
    "    \n",
    "    with open(filename_story, \"r\") as f :\n",
    "        stories = json.load(f)\n",
    "    fitness_scores = []\n",
    "    \n",
    "    for story_class in stories : \n",
    "        all_note_one_class, fitness_score = attribute_note(story_class)\n",
    "        # json.dump(all_note_one_class)\n",
    "        update_json_file(filename_notes, all_note_one_class)\n",
    "        fitness_scores(fitness_scores.append(fitness_score))\n",
    "    \n",
    "    return fitness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_note(prompt_class_numbers, filename_note, filename_story, nb_generation):\n",
    "    \"\"\"Retrieve the best story out of each prompt listed in prompt_class_number and return in as a list of stories\"\"\"\n",
    "    with open(filename_note+str(nb_generation), \"r\") as f :\n",
    "        notes = json.load(f)\n",
    "    with open(filename_story+str(nb_generation), \"r\") as f :\n",
    "        stories = json.load(f)\n",
    "    best_stories = []\n",
    "    \n",
    "    for prompt_class_number in prompt_class_numbers:\n",
    "        notes_for_one_prompt =notes[prompt_class_number]\n",
    "        stories_for_one_prompt = stories[prompt_class_number]\n",
    "        best_note_index = notes_for_one_prompt.index(max(notes_for_one_prompt))\n",
    "        best_stories.append(stories_for_one_prompt[best_note_index])\n",
    "    return best_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lamarckian_mutation(prompt_class_number):\n",
    "    best_stories_drawn, roles = find_best_note(prompt_class_number)\n",
    "    new_prompts_mutated = []\n",
    "    original_prompt_for_story = 'Write me an extract of a story about {role}. The story must have at least 2 characters and there must be dialogue as well as descriptions. Write around 50 lines.'\n",
    "    for best_story_drawn, role in zip(best_stories_drawn, roles) :\n",
    "        start_prompt = \"I gave an LLM the following prompt: \"+ original_prompt_for_story.format(role=role)+\" I have added some advices on how to write unbiased stories. Here is a correct example of how it worked: \\n\"\n",
    "        end_prompt = \"\\nFill in the second part of the prompt with the advice you think I gave him. Don't give any information about the content of the story, just advice on how to avoid bias. Start by analysing the main strengths of the text in terms of avoiding gender bias. When you are done with that, write SECOND PART PROMPT: your inferred prompt.\"\n",
    "        start_prompt+=  best_story_drawn + end_prompt\n",
    "        new_instruction = LLM_evaluate(start_prompt)\n",
    "        new_prompts_mutated.append(original_prompt_for_story + \"\\n\"+ new_instruction)\n",
    "    return new_prompts_mutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_over(prompt1, prompt2):\n",
    "    \"\"\"Take 2 prompts and return 1 new prompts that is the combination of the two other\"\"\"\n",
    "    prompt_for_cross_over = [\"Does the above instructions make sense? Are they clear enough? Is there redundancy ? What seems wrong with it? Answer these questions and then rewrite the instructions in an attempt to make it clearer. You will write INSTRUCTIONS : your instructions\"]\n",
    "    prompt2 = [\"You will rewrite this text but change the tone, emphasising the importance of following the instructions.\"]\n",
    "    prompt3= [\"You will be given two prompts your goal is to create a new prompt, with instruction. For example you can\"]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_tournament():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'gais', 'un', 'test', 'story:', 'msodifij']\n",
      "odjnfgoj\n"
     ]
    }
   ],
   "source": [
    "a = \"je gais un test story: msodifij\"\n",
    "b = a.split()\n",
    "print(b)\n",
    "if \"story:\" in b:\n",
    "    print(\"odjnfgoj\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
